# ã‚¢ãƒ—ãƒ­ãƒ¼ãƒAè©³ç´°å®Ÿè£…ã‚¬ã‚¤ãƒ‰: Cloud Scheduler + HTTP Endpoint

## ğŸ“‹ ç›®æ¬¡

1. [æ¦‚è¦](#æ¦‚è¦)
2. [å®Ÿè£…ã®å…¨ä½“åƒ](#å®Ÿè£…ã®å…¨ä½“åƒ)
3. [è©³ç´°å®Ÿè£…æ‰‹é †](#è©³ç´°å®Ÿè£…æ‰‹é †)
4. [ã‚³ãƒ¼ãƒ‰å®Ÿè£…ä¾‹](#ã‚³ãƒ¼ãƒ‰å®Ÿè£…ä¾‹)
5. [ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š](#ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š)
6. [ãƒ†ã‚¹ãƒˆæ–¹æ³•](#ãƒ†ã‚¹ãƒˆæ–¹æ³•)
7. [é‹ç”¨ãƒ»ç›£è¦–](#é‹ç”¨ç›£è¦–)
8. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)

---

## æ¦‚è¦

### ãªãœã‚¢ãƒ—ãƒ­ãƒ¼ãƒAãŒæœ€é©ã‹

**Cloud Scheduler + HTTP Endpoint** ã¯ã€ä»¥ä¸‹ã®ç†ç”±ã§è‚²æˆæ—¥è¨˜è‡ªå‹•ç”Ÿæˆã«æœ€é©ã§ã™ï¼š

1. **æ—¢å­˜ã‚¤ãƒ³ãƒ•ãƒ©ã®æ´»ç”¨**
   - æ—¢ã«Cloud Runä¸Šã§FastAPIãŒç¨¼åƒä¸­
   - æ–°ã—ã„ã‚µãƒ¼ãƒ“ã‚¹ã‚„ã‚¤ãƒ³ãƒ•ãƒ©ä¸è¦
   - ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ•ãƒ­ãƒ¼å¤‰æ›´ãªã—

2. **ã‚·ãƒ³ãƒ—ãƒ«ãªæ§‹æˆ**
   - HTTP POSTãƒªã‚¯ã‚¨ã‚¹ãƒˆ1ã¤ã§ãƒˆãƒªã‚¬ãƒ¼
   - Background Tasksã§éåŒæœŸå‡¦ç†
   - Cloud Loggingã§ä¸€å…ƒç®¡ç†

3. **ä½ã‚³ã‚¹ãƒˆ**
   - Cloud Scheduler: $0.10/æœˆ
   - Vertex AI Gemini 3 Pro: ~$1-2/æœˆ
   - è¿½åŠ ã®Computeè²»ç”¨: ã»ã¼ãªã—

4. **é«˜ã„ä¿¡é ¼æ€§**
   - Googleãƒãƒãƒ¼ã‚¸ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹
   - è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½
   - SLA 99.9%

### ã‚·ã‚¹ãƒ†ãƒ ãƒ•ãƒ­ãƒ¼è©³ç´°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Cloud Scheduler                              â”‚
â”‚  ãƒ»æ¯æ—¥23:50 JSTï¼ˆAsia/Tokyoï¼‰                                   â”‚
â”‚  ãƒ»Cronå¼: "50 23 * * *"                                        â”‚
â”‚  ãƒ»ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³: Asia/Tokyo                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ HTTP POST /api/diary/generate-daily
             â”‚ Authorization: Bearer [OIDC Token]
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FastAPI (Cloud Run)                            â”‚
â”‚                                                                   â”‚
â”‚  1. OIDC ãƒˆãƒ¼ã‚¯ãƒ³æ¤œè¨¼                                            â”‚
â”‚     â””â”€ ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç¢ºèª                                    â”‚
â”‚                                                                   â”‚
â”‚  2. å¯¾è±¡æ—¥ä»˜è¨ˆç®—                                                 â”‚
â”‚     â””â”€ å‰æ—¥ã®æ—¥ä»˜ã‚’å–å¾—ï¼ˆ23:50å®Ÿè¡Œã®ãŸã‚ï¼‰                      â”‚
â”‚                                                                   â”‚
â”‚  3. Background Task ã‚­ãƒ¥ãƒ¼                                       â”‚
â”‚     â””â”€ å³åº§ã«HTTP 202 Acceptedã‚’è¿”å´                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ Background Task
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Diary Generation Service                            â”‚
â”‚                                                                   â”‚
â”‚  Step 1: ãƒ‡ãƒ¼ã‚¿åé›† (5-10ç§’)                                     â”‚
â”‚  â”œâ”€ agent_execution_logs ã‹ã‚‰1æ—¥åˆ†ã®ãƒ­ã‚°å–å¾—                    â”‚
â”‚  â”œâ”€ sensor_logs ã‹ã‚‰1æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿å–å¾—                           â”‚
â”‚  â”œâ”€ vegetables ã‹ã‚‰ç¾åœ¨è‚²æˆä¸­ã®é‡èœæƒ…å ±å–å¾—                     â”‚
â”‚  â””â”€ plant_camera ã‹ã‚‰æœ€æ–°ç”»åƒå–å¾—                               â”‚
â”‚                                                                   â”‚
â”‚  Step 2: ãƒ‡ãƒ¼ã‚¿åŠ å·¥ (1-2ç§’)                                      â”‚
â”‚  â”œâ”€ çµ±è¨ˆè¨ˆç®—ï¼ˆæ¸©åº¦ãƒ»æ¹¿åº¦ãƒ»åœŸå£Œæ°´åˆ†ã® min/max/avgï¼‰              â”‚
â”‚  â”œâ”€ ä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡ºï¼ˆãƒ‡ãƒã‚¤ã‚¹æ“ä½œã€è­¦å‘Šã€ã‚¢ãƒ©ãƒ¼ãƒˆï¼‰            â”‚
â”‚  â””â”€ ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³æ•´ç†                                            â”‚
â”‚                                                                   â”‚
â”‚  Step 3: AIæ—¥è¨˜ç”Ÿæˆ (10-15ç§’)                                    â”‚
â”‚  â”œâ”€ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰                                              â”‚
â”‚  â”œâ”€ Vertex AI Gemini 3 Pro å‘¼ã³å‡ºã—                            â”‚
â”‚  â”œâ”€ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‘ãƒ¼ã‚¹ï¼ˆJSONæŠ½å‡ºï¼‰                                â”‚
â”‚  â””â”€ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆ429å¯¾å¿œã®æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ãƒªãƒˆãƒ©ã‚¤ï¼‰      â”‚
â”‚                                                                   â”‚
â”‚  Step 4: ä¿å­˜ (1ç§’)                                              â”‚
â”‚  â””â”€ Firestore growing_diaries ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ä¿å­˜               â”‚
â”‚                                                                   â”‚
â”‚  åˆè¨ˆæ‰€è¦æ™‚é–“: ç´„20-30ç§’                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Vertex AI Gemini 3 Proã®åˆ©ç‚¹

ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã¯ **Vertex AI Gemini 3 Pro** ã‚’ä½¿ç”¨ã—ã¾ã™ï¼š

1. **ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰ã®ä¿¡é ¼æ€§**
   - Google Cloudçµ±åˆã«ã‚ˆã‚‹SLAä¿è¨¼
   - è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¨ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°
   - ãƒªãƒ¼ã‚¸ãƒ§ãƒŠãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå¯¾å¿œ

2. **çµ±ä¸€ã•ã‚ŒãŸã‚¢ã‚¯ã‚»ã‚¹ç®¡ç†**
   - Google Cloud IAMã«ã‚ˆã‚‹æ¨©é™ç®¡ç†
   - ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã®èªè¨¼
   - API Keyã§ã¯ãªãã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³èªè¨¼

3. **é«˜åº¦ãªã‚¯ã‚©ãƒ¼ã‚¿ç®¡ç†**
   - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå˜ä½ã§ã®ã‚¯ã‚©ãƒ¼ã‚¿ç®¡ç†
   - Cloud Consoleã‹ã‚‰ç°¡å˜ã«ç¢ºèªãƒ»å¢—åŠ å¯èƒ½
   - ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å¯è¦–åŒ–

4. **ã‚³ã‚¹ãƒˆæœ€é©åŒ–**
   - ä½¿ç”¨é‡ã«å¿œã˜ãŸèª²é‡‘
   - ç„¡æ–™æ ã®æ´»ç”¨
   - è©³ç´°ãªä½¿ç”¨çŠ¶æ³ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°

5. **429ã‚¨ãƒ©ãƒ¼å¯¾ç­–ã®é‡è¦æ€§**
   - ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã¯é¿ã‘ã‚‰ã‚Œãªã„å ´åˆãŒã‚ã‚‹
   - æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ãƒªãƒˆãƒ©ã‚¤ã§è‡ªå‹•å¾©æ—§
   - ãƒ©ãƒ³ãƒ€ãƒ ã‚¸ãƒƒã‚¿ãƒ¼ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ†æ•£
   - æœ€å¤§5å›ã®ãƒªãƒˆãƒ©ã‚¤ã§é«˜ã„æˆåŠŸç‡

---

## å®Ÿè£…ã®å…¨ä½“åƒ

### ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
ai-batake-app/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                          # â† æ–°è¦ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¿½åŠ 
â”‚   â”œâ”€â”€ diary_service.py                 # â† æ–°è¦ä½œæˆï¼ˆæ—¥è¨˜ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ï¼‰
â”‚   â”œâ”€â”€ db.py                            # â† æ—¢å­˜ï¼ˆæ—¥è¨˜ä¿å­˜é–¢æ•°è¿½åŠ ï¼‰
â”‚   â”œâ”€â”€ requirements.txt                 # â† ä¾å­˜é–¢ä¿‚è¿½åŠ ã®å¯èƒ½æ€§
â”‚   â””â”€â”€ tests/
â”‚       â””â”€â”€ test_diary_service.py        # â† æ–°è¦ä½œæˆï¼ˆãƒ†ã‚¹ãƒˆï¼‰
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app/
â”‚       â””â”€â”€ diary/
â”‚           â””â”€â”€ page.tsx                 # â† æ–°è¦ä½œæˆï¼ˆæ—¥è¨˜ä¸€è¦§ãƒšãƒ¼ã‚¸ï¼‰
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup_cloud_scheduler.sh         # â† æ–°è¦ä½œæˆï¼ˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼‰
â”‚
â””â”€â”€ docs/
    â””â”€â”€ APPROACH_A_DETAILED_GUIDE.md     # â† ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
```

### å¿…è¦ãªæ¨©é™

#### GCP IAMæ¨©é™

1. **Cloud Schedulerç”¨ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ**
   - `roles/run.invoker` - Cloud Runã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå‘¼ã³å‡ºã—

2. **Cloud Runç”¨ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ**ï¼ˆæ—¢å­˜ï¼‰
   - `roles/datastore.user` - Firestoreèª­ã¿æ›¸ã
   - `roles/aiplatform.user` - Vertex AI APIåˆ©ç”¨ï¼ˆGemini 3 Proï¼‰

---

## è©³ç´°å®Ÿè£…æ‰‹é †

### Phase 1: ã‚¤ãƒ³ãƒ•ãƒ©è¨­å®šï¼ˆ30åˆ†ï¼‰

#### 1.1 ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆ

```bash
#!/bin/bash
# scripts/setup_cloud_scheduler.sh

PROJECT_ID="ai-agentic-hackathon-4"
REGION="us-central1"
SERVICE_NAME="ai-batake-app"

echo "Step 1: Creating service account for Cloud Scheduler..."

# ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆ
gcloud iam service-accounts create scheduler-invoker \
    --display-name="Cloud Scheduler Service Account for Diary Generation" \
    --description="Service account used by Cloud Scheduler to invoke diary generation endpoint" \
    --project=$PROJECT_ID

# ä½œæˆç¢ºèª
gcloud iam service-accounts list \
    --filter="email:scheduler-invoker@${PROJECT_ID}.iam.gserviceaccount.com" \
    --project=$PROJECT_ID
```

#### 1.2 æ¨©é™ä»˜ä¸

```bash
echo "Step 2: Granting Cloud Run Invoker role..."

# Cloud Run Invoker ãƒ­ãƒ¼ãƒ«ã‚’ä»˜ä¸
gcloud run services add-iam-policy-binding $SERVICE_NAME \
    --member="serviceAccount:scheduler-invoker@${PROJECT_ID}.iam.gserviceaccount.com" \
    --role="roles/run.invoker" \
    --region=$REGION \
    --project=$PROJECT_ID

# æ¨©é™ç¢ºèª
gcloud run services get-iam-policy $SERVICE_NAME \
    --region=$REGION \
    --project=$PROJECT_ID
```

#### 1.3 Cloud Scheduler ã‚¸ãƒ§ãƒ–ä½œæˆ

```bash
echo "Step 3: Creating Cloud Scheduler job..."

# ã‚µãƒ¼ãƒ“ã‚¹URLã‚’å–å¾—
SERVICE_URL=$(gcloud run services describe $SERVICE_NAME \
    --region=$REGION \
    --project=$PROJECT_ID \
    --format='value(status.url)')

echo "Service URL: $SERVICE_URL"

# Schedulerã‚¸ãƒ§ãƒ–ä½œæˆ
gcloud scheduler jobs create http daily-diary-generator \
    --schedule="50 23 * * *" \
    --uri="${SERVICE_URL}/api/diary/generate-daily" \
    --http-method=POST \
    --oidc-service-account-email=scheduler-invoker@${PROJECT_ID}.iam.gserviceaccount.com \
    --location=$REGION \
    --time-zone="Asia/Tokyo" \
    --description="Daily growing diary generation at 23:50 JST" \
    --attempt-deadline=300s \
    --max-retry-attempts=3 \
    --min-backoff=5s \
    --max-backoff=60s \
    --project=$PROJECT_ID

echo "âœ“ Cloud Scheduler job created successfully!"
```

#### 1.4 è¨­å®šç¢ºèª

```bash
echo "Step 4: Verifying configuration..."

# ã‚¸ãƒ§ãƒ–ä¸€è¦§ç¢ºèª
gcloud scheduler jobs list \
    --location=$REGION \
    --project=$PROJECT_ID

# ã‚¸ãƒ§ãƒ–è©³ç´°ç¢ºèª
gcloud scheduler jobs describe daily-diary-generator \
    --location=$REGION \
    --project=$PROJECT_ID
```

### Phase 2: ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å®Ÿè£…ï¼ˆ2-3æ—¥ï¼‰

#### 2.1 diary_service.py ä½œæˆ

```python
# backend/diary_service.py
"""
è‚²æˆæ—¥è¨˜ç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ï¼š
1. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ­ã‚°ã¨ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®åé›†
2. çµ±è¨ˆæƒ…å ±ã®è¨ˆç®—
3. Vertex AI Gemini 3 Proã‚’ä½¿ç”¨ã—ãŸæ—¥è¨˜ç”Ÿæˆ
4. Firestoreã¸ã®ä¿å­˜
"""

import os
import logging
import json
import time
import random
from datetime import datetime, timedelta, date as date_type
from typing import Dict, List, Any, Optional
from google.cloud import firestore
import google.auth
from google.auth.transport.requests import Request
import requests

# Import from existing modules
try:
    from .db import db
except ImportError:
    from db import db

# Configuration
PROJECT_ID = os.environ.get("GOOGLE_CLOUD_PROJECT", "ai-agentic-hackathon-4")
LOCATION = "us-central1"
GEMINI_MODEL = "gemini-3-flash-preview"  # Vertex AI Gemini 3 Pro
VERTEX_AI_ENDPOINT = f"https://{LOCATION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models/{GEMINI_MODEL}:generateContent"

logger = logging.getLogger(__name__)


def get_vertex_ai_access_token():
    """
    Vertex AIç”¨ã®ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
    
    Returns:
        ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³æ–‡å­—åˆ—
    """
    credentials, _ = google.auth.default()
    credentials.refresh(Request())
    return credentials.token


# ============================================================
# ãƒ‡ãƒ¼ã‚¿åé›†é–¢æ•°
# ============================================================

async def collect_daily_data(target_date: date_type) -> Dict[str, Any]:
    """
    æŒ‡å®šæ—¥ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
    
    Args:
        target_date: å¯¾è±¡æ—¥ä»˜
    
    Returns:
        åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸
        {
            "date": "2025-02-04",
            "agent_logs": [...],
            "sensor_data": [...],
            "vegetable": {...},
            "plant_image": "https://..."
        }
    """
    logger.info(f"Starting data collection for {target_date}")
    
    # æ—¥ä»˜ç¯„å›²ã®è¨­å®šï¼ˆ0:00:00 - 23:59:59ï¼‰
    start_time = datetime.combine(target_date, datetime.min.time())
    end_time = datetime.combine(target_date, datetime.max.time())
    
    # ä¸¦è¡Œã§ãƒ‡ãƒ¼ã‚¿å–å¾—
    agent_logs = await get_agent_logs_for_date(start_time, end_time)
    sensor_data = await get_sensor_data_for_date(start_time, end_time)
    current_vegetable = await get_current_vegetable()
    plant_image = await get_plant_image_for_date(target_date)
    
    logger.info(f"Data collection complete: {len(agent_logs)} agent logs, {len(sensor_data)} sensor readings")
    
    return {
        "date": target_date.isoformat(),
        "agent_logs": agent_logs,
        "sensor_data": sensor_data,
        "vegetable": current_vegetable,
        "plant_image": plant_image
    }


async def get_agent_logs_for_date(start: datetime, end: datetime) -> List[Dict]:
    """æŒ‡å®šæœŸé–“ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ­ã‚°ã‚’å–å¾—"""
    if db is None:
        logger.warning("Database not available")
        return []
    
    try:
        # Firestoreã‚¯ã‚¨ãƒª: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ç¯„å›²æ¤œç´¢
        docs = db.collection("agent_execution_logs") \
            .where("timestamp", ">=", start.isoformat()) \
            .where("timestamp", "<=", end.isoformat()) \
            .order_by("timestamp") \
            .stream()
        
        logs = []
        for doc in docs:
            log_data = doc.to_dict()
            log_data['id'] = doc.id
            logs.append(log_data)
        
        logger.info(f"Retrieved {len(logs)} agent logs")
        return logs
        
    except Exception as e:
        logger.error(f"Error fetching agent logs: {e}", exc_info=True)
        return []


async def get_sensor_data_for_date(start: datetime, end: datetime) -> List[Dict]:
    """æŒ‡å®šæœŸé–“ã®ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    if db is None:
        logger.warning("Database not available")
        return []
    
    try:
        # Unix timestampã«å¤‰æ›
        start_unix = int(start.timestamp())
        end_unix = int(end.timestamp())
        
        # Firestoreã‚¯ã‚¨ãƒª
        docs = db.collection("sensor_logs") \
            .where("unix_timestamp", ">=", start_unix) \
            .where("unix_timestamp", "<=", end_unix) \
            .order_by("unix_timestamp") \
            .stream()
        
        data = []
        for doc in docs:
            sensor_log = doc.to_dict()
            sensor_log['id'] = doc.id
            data.append(sensor_log)
        
        logger.info(f"Retrieved {len(data)} sensor readings")
        return data
        
    except Exception as e:
        logger.error(f"Error fetching sensor data: {e}", exc_info=True)
        return []


async def get_current_vegetable() -> Optional[Dict]:
    """ç¾åœ¨è‚²æˆä¸­ã®é‡èœæƒ…å ±ã‚’å–å¾—"""
    if db is None:
        return None
    
    try:
        # edge_agentè¨­å®šã‹ã‚‰å–å¾—ã™ã‚‹ã‹ã€æœ€æ–°ã®é‡èœã‚’å–å¾—
        docs = db.collection("vegetables") \
            .where("status", "==", "completed") \
            .order_by("created_at", direction=firestore.Query.DESCENDING) \
            .limit(1) \
            .stream()
        
        for doc in docs:
            veg_data = doc.to_dict()
            veg_data['id'] = doc.id
            return veg_data
        
        return None
        
    except Exception as e:
        logger.error(f"Error fetching vegetable info: {e}", exc_info=True)
        return None


async def get_plant_image_for_date(target_date: date_type) -> Optional[str]:
    """æŒ‡å®šæ—¥ã®æ¤ç‰©ç”»åƒURLã‚’å–å¾—"""
    if db is None:
        return None
    
    try:
        # plant_cameraã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰æœ€æ–°ç”»åƒã‚’å–å¾—
        # å®Ÿè£…ã¯æ—¢å­˜ã®get_latest_plant_cameraé–¢æ•°ã‚’å‚ç…§
        # ã“ã“ã§ã¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
        return None
        
    except Exception as e:
        logger.error(f"Error fetching plant image: {e}", exc_info=True)
        return None


# ============================================================
# ãƒ‡ãƒ¼ã‚¿åŠ å·¥é–¢æ•°
# ============================================================

def calculate_statistics(sensor_data: List[Dict]) -> Dict:
    """
    ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çµ±è¨ˆã‚’è¨ˆç®—
    
    Args:
        sensor_data: ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
    
    Returns:
        çµ±è¨ˆæƒ…å ±ã®è¾æ›¸
    """
    if not sensor_data:
        logger.warning("No sensor data available for statistics")
        return {
            "temperature": {"min": 0, "max": 0, "avg": 0},
            "humidity": {"min": 0, "max": 0, "avg": 0},
            "soil_moisture": {"min": 0, "max": 0, "avg": 0},
        }
    
    # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
    temps = [d.get("temperature", 0) for d in sensor_data if d.get("temperature") is not None]
    humids = [d.get("humidity", 0) for d in sensor_data if d.get("humidity") is not None]
    soils = [d.get("soil_moisture", 0) for d in sensor_data if d.get("soil_moisture") is not None]
    
    # çµ±è¨ˆè¨ˆç®—
    stats = {
        "temperature": {
            "min": round(min(temps), 1) if temps else 0,
            "max": round(max(temps), 1) if temps else 0,
            "avg": round(sum(temps) / len(temps), 1) if temps else 0,
        },
        "humidity": {
            "min": round(min(humids), 1) if humids else 0,
            "max": round(max(humids), 1) if humids else 0,
            "avg": round(sum(humids) / len(humids), 1) if humids else 0,
        },
        "soil_moisture": {
            "min": round(min(soils), 1) if soils else 0,
            "max": round(max(soils), 1) if soils else 0,
            "avg": round(sum(soils) / len(soils), 1) if soils else 0,
        },
    }
    
    logger.info(f"Statistics calculated: temp {stats['temperature']['avg']}Â°C, humidity {stats['humidity']['avg']}%")
    
    return stats


def extract_key_events(agent_logs: List[Dict], max_events: int = 15) -> List[Dict]:
    """
    é‡è¦ãªã‚¤ãƒ™ãƒ³ãƒˆã‚’æŠ½å‡º
    
    Args:
        agent_logs: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ­ã‚°ã®ãƒªã‚¹ãƒˆ
        max_events: æœ€å¤§ã‚¤ãƒ™ãƒ³ãƒˆæ•°
    
    Returns:
        ã‚¤ãƒ™ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    events = []
    
    for log in agent_logs:
        log_data = log.get("data", {})
        timestamp = log.get("timestamp", "")
        
        # æ“ä½œã‚¤ãƒ™ãƒ³ãƒˆï¼ˆãƒ‡ãƒã‚¤ã‚¹åˆ¶å¾¡ï¼‰
        if "operation" in log_data:
            for device, op in log_data["operation"].items():
                action = op.get("action", "")
                
                # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªæ“ä½œã®ã¿æŠ½å‡º
                if any(keyword in action for keyword in ["ON", "OFF", "èµ·å‹•", "åœæ­¢", "å¤‰æ›´"]):
                    events.append({
                        "time": timestamp,
                        "type": "action",
                        "device": device,
                        "action": action
                    })
        
        # è­¦å‘Šãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆ
        comment = log_data.get("comment", "")
        if "ç•°å¸¸" in comment or "ã‚¨ãƒ©ãƒ¼" in comment:
            events.append({
                "time": timestamp,
                "type": "alert",
                "action": comment
            })
        elif "è­¦å‘Š" in comment or "æ³¨æ„" in comment:
            events.append({
                "time": timestamp,
                "type": "warning",
                "action": comment
            })
    
    # æœ€å¤§ä»¶æ•°ã¾ã§ï¼ˆæ–°ã—ã„ã‚‚ã®ã‹ã‚‰ï¼‰
    events_sorted = sorted(events, key=lambda x: x.get("time", ""), reverse=True)
    result = events_sorted[:max_events]
    
    logger.info(f"Extracted {len(result)} key events from {len(agent_logs)} logs")
    
    return result


# ============================================================
# AIæ—¥è¨˜ç”Ÿæˆ
# ============================================================

def build_diary_prompt(
    date_str: str,
    statistics: Dict,
    events: List[Dict],
    vegetable_info: Optional[Dict]
) -> str:
    """
    æ—¥è¨˜ç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
    
    Args:
        date_str: æ—¥ä»˜æ–‡å­—åˆ—
        statistics: çµ±è¨ˆæƒ…å ±
        events: ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        vegetable_info: é‡èœæƒ…å ±
    
    Returns:
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—
    """
    veg_name = vegetable_info.get("name", "é‡èœ") if vegetable_info else "é‡èœ"
    
    # ã‚¤ãƒ™ãƒ³ãƒˆè¦ç´„ï¼ˆæ™‚åˆ»é †ã«ä¸¦ã³æ›¿ãˆï¼‰
    events_sorted = sorted(events, key=lambda x: x.get("time", ""))
    event_lines = []
    for e in events_sorted[:10]:  # æœ€å¤§10ä»¶
        time_str = e.get("time", "")
        try:
            # ISOå½¢å¼ã‹ã‚‰HH:MMã«å¤‰æ›
            dt = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
            time_display = dt.strftime("%H:%M")
        except:
            time_display = time_str
        
        device = e.get("device", "")
        action = e.get("action", "")
        event_lines.append(f"- {time_display}: {device} {action}" if device else f"- {time_display}: {action}")
    
    event_summary = "\n".join(event_lines) if event_lines else "ç‰¹ã«ãªã—"
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    prompt = f"""ã‚ãªãŸã¯æ¤ç‰©æ ½åŸ¹ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚‚ã¨ã«ã€è‚²æˆæ—¥è¨˜ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€æ—¥ä»˜ã€‘
{date_str}

ã€è‚²æˆä¸­ã®æ¤ç‰©ã€‘
{veg_name}

ã€ç’°å¢ƒãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã€‘
æ¸©åº¦: æœ€ä½ {statistics['temperature']['min']}Â°C / æœ€é«˜ {statistics['temperature']['max']}Â°C / å¹³å‡ {statistics['temperature']['avg']}Â°C
æ¹¿åº¦: æœ€ä½ {statistics['humidity']['min']}% / æœ€é«˜ {statistics['humidity']['max']}% / å¹³å‡ {statistics['humidity']['avg']}%
åœŸå£Œæ°´åˆ†: æœ€ä½ {statistics['soil_moisture']['min']}% / æœ€é«˜ {statistics['soil_moisture']['max']}% / å¹³å‡ {statistics['soil_moisture']['avg']}%

ã€ä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆã€‘
{event_summary}

ä»¥ä¸‹ã®3ã¤ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†ã‘ã¦æ—¥è¨˜ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

1. **ä»Šæ—¥ã®è¦ç´„** (200-300æ–‡å­—)
   - 1æ—¥ã®ç’°å¢ƒçŠ¶æ…‹ã¨å…¨ä½“çš„ãªæ§˜å­ã‚’è¦ç´„
   - ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹ç‰¹å¾´çš„ãªç‚¹ã‚’è¨˜è¼‰
   - è¦ªã—ã¿ã‚„ã™ã„æ–‡ä½“ã§

2. **æˆé•·è¦³å¯Ÿ** (100-200æ–‡å­—)
   - æ¤ç‰©ã®çŠ¶æ…‹ã«ã¤ã„ã¦æ¨æ¸¬ã•ã‚Œã‚‹è¦³å¯Ÿ
   - ç’°å¢ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åˆ¤æ–­ã§ãã‚‹æˆé•·ã®é€²æ—
   - å…·ä½“çš„ãªè¦³å¯Ÿãƒã‚¤ãƒ³ãƒˆ

3. **æ˜æ—¥ã¸ã®ææ¡ˆ** (100-150æ–‡å­—)
   - ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæ”¹å–„ææ¡ˆ
   - æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚„æ³¨æ„ç‚¹
   - å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹

**é‡è¦**: å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ï¼ˆä»–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯å«ã‚ãªã„ã“ã¨ï¼‰ï¼š
```json
{{
  "summary": "ä»Šæ—¥ã®è¦ç´„æ–‡...",
  "observations": "æˆé•·è¦³å¯Ÿæ–‡...",
  "recommendations": "æ˜æ—¥ã¸ã®ææ¡ˆæ–‡..."
}}
```

æ—¥è¨˜ã¯è¦ªã—ã¿ã‚„ã™ãã€å°‚é–€çš„ã™ããªã„æ–‡ä½“ã§æ›¸ã„ã¦ãã ã•ã„ã€‚
"""
    
    return prompt


async def generate_diary_with_ai(
    date_str: str,
    statistics: Dict,
    events: List[Dict],
    vegetable_info: Optional[Dict],
    max_retries: int = 5
) -> Dict[str, str]:
    """
    Vertex AI Gemini 3 Proã‚’ä½¿ç”¨ã—ã¦æ—¥è¨˜ã‚’ç”Ÿæˆï¼ˆ429å¯¾å¿œã®æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
    
    Args:
        date_str: æ—¥ä»˜æ–‡å­—åˆ—
        statistics: çµ±è¨ˆæƒ…å ±
        events: ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        vegetable_info: é‡èœæƒ…å ±
        max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5å›ï¼‰
    
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸæ—¥è¨˜ã®è¾æ›¸
        {
            "summary": "...",
            "observations": "...",
            "recommendations": "..."
        }
    
    Raises:
        RuntimeError: APIå‘¼ã³å‡ºã—å¤±æ•—æ™‚
    """
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    prompt = build_diary_prompt(date_str, statistics, events, vegetable_info)
    
    # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ï¼ˆVertex AIå½¢å¼ï¼‰
    payload = {
        "contents": [{
            "role": "user",
            "parts": [{
                "text": prompt
            }]
        }],
        "generationConfig": {
            "temperature": 0.7,
            "topP": 0.95,
            "topK": 40,
            "maxOutputTokens": 2048,
            "candidateCount": 1
        },
        "safetySettings": [
            {
                "category": "HARM_CATEGORY_HATE_SPEECH",
                "threshold": "BLOCK_ONLY_HIGH"
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "threshold": "BLOCK_ONLY_HIGH"
            },
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "threshold": "BLOCK_ONLY_HIGH"
            },
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": "BLOCK_ONLY_HIGH"
            }
        ]
    }
    
    # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ãƒªãƒˆãƒ©ã‚¤ãƒ«ãƒ¼ãƒ—
    base_delay = 2
    for attempt in range(max_retries):
        try:
            # ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—ï¼ˆæ¯å›æ›´æ–°ã—ã¦æœ‰åŠ¹æ€§ã‚’ç¢ºä¿ï¼‰
            access_token = get_vertex_ai_access_token()
            
            headers = {
                "Authorization": f"Bearer {access_token}",
                "Content-Type": "application/json"
            }
            
            logger.info(f"Calling Vertex AI Gemini 3 Pro (attempt {attempt + 1}/{max_retries})...")
            
            response = requests.post(
                VERTEX_AI_ENDPOINT,
                headers=headers,
                json=payload,
                timeout=90
            )
            
            if response.status_code == 200:
                result = response.json()
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                if "candidates" in result and len(result["candidates"]) > 0:
                    candidate = result["candidates"][0]
                    if "content" in candidate and "parts" in candidate["content"]:
                        generated_text = candidate["content"]["parts"][0].get("text", "")
                        
                        logger.info(f"Successfully generated diary text ({len(generated_text)} chars)")
                        
                        # ãƒ‘ãƒ¼ã‚¹ã—ã¦JSONæŠ½å‡º
                        return parse_diary_response(generated_text)
                
                raise RuntimeError("Unexpected API response format")
            
            elif response.status_code == 429:
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼: æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§ãƒªãƒˆãƒ©ã‚¤
                # è¨ˆç®—å¼: (base_delay * 2^attempt) + ãƒ©ãƒ³ãƒ€ãƒ ã‚¸ãƒƒã‚¿ãƒ¼
                jitter = random.uniform(0, 1)
                wait_time = (base_delay * (2 ** attempt)) + jitter
                
                logger.warning(
                    f"Rate limit (429) hit. Waiting {wait_time:.2f}s before retry "
                    f"(attempt {attempt + 1}/{max_retries})..."
                )
                
                time.sleep(wait_time)
                continue
            
            elif response.status_code >= 500:
                # ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: ãƒªãƒˆãƒ©ã‚¤
                jitter = random.uniform(0, 1)
                wait_time = (base_delay * (2 ** attempt)) + jitter
                
                logger.warning(
                    f"Server error ({response.status_code}). Waiting {wait_time:.2f}s before retry "
                    f"(attempt {attempt + 1}/{max_retries})..."
                )
                
                time.sleep(wait_time)
                continue
            
            else:
                # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ï¼ˆ400, 403ãªã©ï¼‰: ãƒªãƒˆãƒ©ã‚¤ã—ãªã„
                error_msg = f"API error: {response.status_code} - {response.text}"
                logger.error(error_msg)
                raise RuntimeError(error_msg)
        
        except requests.exceptions.Timeout:
            logger.warning(f"Request timeout (attempt {attempt + 1}/{max_retries})")
            if attempt < max_retries - 1:
                jitter = random.uniform(0, 1)
                wait_time = (base_delay * (2 ** attempt)) + jitter
                time.sleep(wait_time)
                continue
            else:
                logger.error("API request timeout after all retries")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹
                return generate_fallback_diary(date_str, statistics, events)
        
        except Exception as e:
            logger.error(f"Unexpected error during API call: {e}", exc_info=True)
            if attempt == max_retries - 1:
                logger.error(f"Failed to generate diary after {max_retries} attempts")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹
                return generate_fallback_diary(date_str, statistics, events)
            
            jitter = random.uniform(0, 1)
            wait_time = (base_delay * (2 ** attempt)) + jitter
            time.sleep(wait_time)
    
    # å…¨ãƒªãƒˆãƒ©ã‚¤å¤±æ•—: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    logger.error("All retry attempts exhausted, using fallback")
    return generate_fallback_diary(date_str, statistics, events)


def parse_diary_response(text: str) -> Dict[str, str]:
    """
    AIå¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦JSONã‚’æŠ½å‡º
    
    Args:
        text: AIç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ
    
    Returns:
        ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸæ—¥è¨˜ã®è¾æ›¸
    """
    try:
        # JSONã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
        clean_text = text.strip()
        
        # ```json ... ``` ã‚’å‰Šé™¤
        if "```json" in clean_text:
            clean_text = clean_text.split("```json")[1].split("```")[0]
        elif "```" in clean_text:
            clean_text = clean_text.split("```")[1].split("```")[0]
        
        # JSON ãƒ‘ãƒ¼ã‚¹
        parsed = json.loads(clean_text.strip())
        
        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ç¢ºèª
        result = {
            "summary": parsed.get("summary", ""),
            "observations": parsed.get("observations", ""),
            "recommendations": parsed.get("recommendations", "")
        }
        
        # ç©ºæ–‡å­—åˆ—ãƒã‚§ãƒƒã‚¯
        if not all(result.values()):
            raise ValueError("Empty fields in parsed response")
        
        logger.info("Successfully parsed AI response")
        return result
    
    except Exception as e:
        logger.error(f"Failed to parse AI response: {e}")
        logger.debug(f"Raw text: {text[:200]}...")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã¨ã—ã¦ä½¿ç”¨
        return {
            "summary": text[:300] if len(text) <= 300 else text[:297] + "...",
            "observations": "AIã«ã‚ˆã‚‹è©³ç´°ãªè¦³å¯Ÿã‚’ç”Ÿæˆä¸­ã§ã™ã€‚",
            "recommendations": "å¼•ãç¶šãç’°å¢ƒã‚’ç›£è¦–ã—ã¾ã™ã€‚"
        }


def generate_fallback_diary(
    date_str: str,
    statistics: Dict,
    events: List[Dict]
) -> Dict[str, str]:
    """
    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹æ—¥è¨˜ç”Ÿæˆ
    
    Args:
        date_str: æ—¥ä»˜
        statistics: çµ±è¨ˆæƒ…å ±
        events: ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆ
    
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸæ—¥è¨˜
    """
    logger.warning("Using fallback template for diary generation")
    
    temp_avg = statistics['temperature']['avg']
    humid_avg = statistics['humidity']['avg']
    soil_avg = statistics['soil_moisture']['avg']
    
    # ã‚¤ãƒ™ãƒ³ãƒˆæ•°ã‚«ã‚¦ãƒ³ãƒˆ
    action_count = sum(1 for e in events if e.get("type") == "action")
    alert_count = sum(1 for e in events if e.get("type") == "alert")
    
    summary = f"""æœ¬æ—¥ï¼ˆ{date_str}ï¼‰ã®æ ½åŸ¹ç’°å¢ƒã¯ã€å¹³å‡æ°—æ¸©{temp_avg}Â°Cã€æ¹¿åº¦{humid_avg}%ã€åœŸå£Œæ°´åˆ†{soil_avg}%ã§æ¨ç§»ã—ã¾ã—ãŸã€‚
ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹è‡ªå‹•åˆ¶å¾¡ãŒ{action_count}å›å®Ÿè¡Œã•ã‚Œã€ç’°å¢ƒã‚’é©åˆ‡ã«ç®¡ç†ã—ã¾ã—ãŸã€‚"""
    
    if alert_count > 0:
        summary += f" {alert_count}ä»¶ã®è­¦å‘ŠãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã¾ã™ã€‚"
    
    observations = f"""ç¾åœ¨ã®ç’°å¢ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åˆ¤æ–­ã™ã‚‹ã¨ã€æ¤ç‰©ã¯{"è‰¯å¥½" if temp_avg > 20 and temp_avg < 30 else "æ³¨æ„ãŒå¿…è¦"}ãªçŠ¶æ…‹ã§ã™ã€‚
æ¸©åº¦ã¨æ¹¿åº¦ã®ãƒãƒ©ãƒ³ã‚¹ãŒ{"é©åˆ‡" if humid_avg > 50 and humid_avg < 80 else "èª¿æ•´ãŒå¿…è¦"}ã«ä¿ãŸã‚Œã¦ã„ã¾ã™ã€‚"""
    
    recommendations = f"""æ˜æ—¥ã‚‚å¼•ãç¶šãç’°å¢ƒç›£è¦–ã‚’ç¶™ç¶šã—ã¾ã™ã€‚
{"æ°—æ¸©ã®å¤‰å‹•ã«æ³¨æ„ã—ã€" if statistics['temperature']['max'] - statistics['temperature']['min'] > 10 else ""}
é©åˆ‡ãªæ°´åˆ†ç®¡ç†ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"""
    
    return {
        "summary": summary,
        "observations": observations,
        "recommendations": recommendations
    }


# ============================================================
# Firestoreä¿å­˜
# ============================================================

async def init_diary_status(diary_id: str):
    """æ—¥è¨˜ç”Ÿæˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’åˆæœŸåŒ–"""
    if db is None:
        logger.warning("Database not available")
        return
    
    try:
        db.collection("growing_diaries").document(diary_id).set({
            "generation_status": "processing",
            "created_at": datetime.now()
        })
        logger.info(f"Initialized diary status for {diary_id}")
    except Exception as e:
        logger.error(f"Failed to initialize diary status: {e}")


async def save_diary(diary_id: str, data: Dict):
    """æ—¥è¨˜ã‚’Firestoreã«ä¿å­˜"""
    if db is None:
        logger.warning("Database not available, cannot save diary")
        return
    
    try:
        db.collection("growing_diaries").document(diary_id).set(data)
        logger.info(f"Diary saved successfully: {diary_id}")
    except Exception as e:
        logger.error(f"Failed to save diary: {e}")
        raise


async def mark_diary_failed(diary_id: str, error_message: str):
    """æ—¥è¨˜ç”Ÿæˆå¤±æ•—ã‚’ãƒãƒ¼ã‚¯"""
    if db is None:
        return
    
    try:
        db.collection("growing_diaries").document(diary_id).update({
            "generation_status": "failed",
            "error_message": error_message,
            "updated_at": datetime.now()
        })
        logger.error(f"Marked diary as failed: {diary_id} - {error_message}")
    except Exception as e:
        logger.error(f"Failed to mark diary as failed: {e}")


# ============================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ============================================================

async def process_daily_diary(target_date_str: str):
    """
    æ—¥è¨˜ç”Ÿæˆã®ãƒ¡ã‚¤ãƒ³å‡¦ç†
    
    Args:
        target_date_str: å¯¾è±¡æ—¥ä»˜ï¼ˆISO 8601å½¢å¼ï¼‰
    
    ã“ã®é–¢æ•°ã¯Background Taskã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚
    """
    start_time = time.time()
    diary_id = target_date_str
    
    try:
        logger.info(f"=== Starting diary generation for {target_date_str} ===")
        
        # æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹
        target_date = date_type.fromisoformat(target_date_str)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆæœŸåŒ–
        await init_diary_status(diary_id)
        
        # Step 1: ãƒ‡ãƒ¼ã‚¿åé›†
        logger.info("Step 1: Collecting daily data...")
        daily_data = await collect_daily_data(target_date)
        
        # Step 2: çµ±è¨ˆè¨ˆç®—ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡º
        logger.info("Step 2: Calculating statistics and extracting events...")
        statistics = calculate_statistics(daily_data["sensor_data"])
        events = extract_key_events(daily_data["agent_logs"])
        
        # Step 3: AIæ—¥è¨˜ç”Ÿæˆ
        logger.info("Step 3: Generating diary with AI...")
        ai_content = await generate_diary_with_ai(
            target_date_str,
            statistics,
            events,
            daily_data["vegetable"]
        )
        
        # Step 4: ä¿å­˜
        logger.info("Step 4: Saving diary to Firestore...")
        generation_time_ms = int((time.time() - start_time) * 1000)
        
        diary_data = {
            "date": target_date_str,
            "created_at": datetime.now(),
            "vegetable_id": daily_data["vegetable"].get("id") if daily_data["vegetable"] else None,
            "vegetable_name": daily_data["vegetable"].get("name") if daily_data["vegetable"] else None,
            "statistics": statistics,
            "events": events,
            "ai_summary": ai_content["summary"],
            "observations": ai_content["observations"],
            "recommendations": ai_content["recommendations"],
            "plant_image_url": daily_data["plant_image"],
            "generation_status": "completed",
            "generation_time_ms": generation_time_ms
        }
        
        await save_diary(diary_id, diary_data)
        
        logger.info(f"=== Diary generation completed successfully in {generation_time_ms}ms ===")
        
    except Exception as e:
        logger.error(f"=== Diary generation failed: {e} ===", exc_info=True)
        await mark_diary_failed(diary_id, str(e))
        raise
```

#### 2.2 main.py ã«ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¿½åŠ 

```python
# backend/main.py ã®æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã«ä»¥ä¸‹ã‚’è¿½åŠ 

from fastapi import BackgroundTasks, Request, HTTPException
from google.oauth2 import id_token
from google.auth.transport import requests as google_requests
from datetime import datetime, timedelta

# Import diary service
try:
    from .diary_service import process_daily_diary
except ImportError:
    from diary_service import process_daily_diary


def verify_scheduler_token(request: Request):
    """
    Cloud Schedulerã‹ã‚‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æ¤œè¨¼
    
    OIDCãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€æ­£ã—ã„ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‹ã‚‰ã®
    ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
    
    Args:
        request: FastAPI Request ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    
    Raises:
        HTTPException: èªè¨¼å¤±æ•—æ™‚
    """
    try:
        # Authorizationãƒ˜ãƒƒãƒ€ãƒ¼å–å¾—
        auth_header = request.headers.get('Authorization', '')
        if not auth_header.startswith('Bearer '):
            raise HTTPException(status_code=403, detail="Missing or invalid Authorization header")
        
        # ãƒˆãƒ¼ã‚¯ãƒ³æŠ½å‡º
        token = auth_header.replace('Bearer ', '')
        
        # OIDCãƒˆãƒ¼ã‚¯ãƒ³æ¤œè¨¼
        claim = id_token.verify_oauth2_token(
            token,
            google_requests.Request()
        )
        
        # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç¢ºèª
        expected_email = "scheduler-invoker@ai-agentic-hackathon-4.iam.gserviceaccount.com"
        if claim.get('email') != expected_email:
            logging.warning(f"Unauthorized service account: {claim.get('email')}")
            raise HTTPException(status_code=403, detail="Unauthorized service account")
        
        logging.info(f"Verified request from: {claim.get('email')}")
        
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Token verification failed: {e}")
        raise HTTPException(status_code=403, detail=f"Invalid token: {str(e)}")


@app.post("/api/diary/generate-daily")
async def generate_daily_diary(
    request: Request,
    background_tasks: BackgroundTasks
):
    """
    æ—¥æ¬¡æ—¥è¨˜ç”Ÿæˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    
    Cloud Schedulerã‹ã‚‰æ¯æ—¥23:50ã«å‘¼ã³å‡ºã•ã‚Œã¾ã™ã€‚
    å‰æ—¥ã®æ—¥è¨˜ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    
    Returns:
        202 Accepted: ã‚¸ãƒ§ãƒ–ãŒå—ã‘ä»˜ã‘ã‚‰ã‚Œã¾ã—ãŸ
        403 Forbidden: èªè¨¼å¤±æ•—
        500 Internal Server Error: ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
    """
    # Cloud Schedulerã‹ã‚‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹æ¤œè¨¼
    verify_scheduler_token(request)
    
    # å‰æ—¥ã®æ—¥ä»˜ã‚’è¨ˆç®—ï¼ˆ23:50å®Ÿè¡Œã®ãŸã‚ï¼‰
    target_date = (datetime.now() - timedelta(hours=1)).date()
    
    logging.info(f"Diary generation request accepted for {target_date.isoformat()}")
    
    # Background Taskã¨ã—ã¦ã‚­ãƒ¥ãƒ¼
    background_tasks.add_task(
        process_daily_diary,
        target_date.isoformat()
    )
    
    return {
        "status": "accepted",
        "date": target_date.isoformat(),
        "message": "Diary generation started in background"
    }


@app.post("/api/diary/generate-manual")
async def generate_manual_diary(
    background_tasks: BackgroundTasks,
    date: str
):
    """
    æ‰‹å‹•æ—¥è¨˜ç”Ÿæˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆãƒ†ã‚¹ãƒˆãƒ»å†ç”Ÿæˆç”¨ï¼‰
    
    Args:
        date: å¯¾è±¡æ—¥ä»˜ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰
    
    Returns:
        202 Accepted: ã‚¸ãƒ§ãƒ–ãŒå—ã‘ä»˜ã‘ã‚‰ã‚Œã¾ã—ãŸ
        400 Bad Request: æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ãƒ©ãƒ¼
    """
    try:
        # æ—¥ä»˜ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        from datetime import date as date_module
        target_date = date_module.fromisoformat(date)
        
        logging.info(f"Manual diary generation request for {date}")
        
        background_tasks.add_task(
            process_daily_diary,
            date
        )
        
        return {
            "status": "accepted",
            "date": date,
            "message": "Manual diary generation started"
        }
    
    except ValueError:
        raise HTTPException(
            status_code=400,
            detail="Invalid date format. Use YYYY-MM-DD"
        )


@app.get("/api/diary/list")
async def list_diaries(limit: int = 30, offset: int = 0):
    """
    è‚²æˆæ—¥è¨˜ä¸€è¦§å–å¾—
    
    Args:
        limit: å–å¾—ä»¶æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30ï¼‰
        offset: ã‚ªãƒ•ã‚»ãƒƒãƒˆ
    
    Returns:
        æ—¥è¨˜ã®ãƒªã‚¹ãƒˆ
    """
    if db is None:
        return {"diaries": []}
    
    try:
        docs = db.collection("growing_diaries") \
            .where("generation_status", "==", "completed") \
            .order_by("date", direction=firestore.Query.DESCENDING) \
            .limit(limit) \
            .offset(offset) \
            .stream()
        
        diaries = []
        for doc in docs:
            diary = doc.to_dict()
            diary['id'] = doc.id
            diaries.append(diary)
        
        return {"diaries": diaries, "count": len(diaries)}
    
    except Exception as e:
        logging.error(f"Error fetching diaries: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/diary/{date}")
async def get_diary(date: str):
    """
    ç‰¹å®šæ—¥ã®æ—¥è¨˜å–å¾—
    
    Args:
        date: æ—¥ä»˜ï¼ˆYYYY-MM-DDï¼‰
    
    Returns:
        æ—¥è¨˜ãƒ‡ãƒ¼ã‚¿
    """
    if db is None:
        raise HTTPException(status_code=503, detail="Database unavailable")
    
    try:
        doc = db.collection("growing_diaries").document(date).get()
        
        if not doc.exists:
            raise HTTPException(status_code=404, detail="Diary not found")
        
        diary = doc.to_dict()
        diary['id'] = doc.id
        return diary
    
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Error fetching diary: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

### Phase 3: ãƒ†ã‚¹ãƒˆï¼ˆ1æ—¥ï¼‰

#### 3.1 æ‰‹å‹•ãƒˆãƒªã‚¬ãƒ¼ãƒ†ã‚¹ãƒˆ

```bash
# Cloud Schedulerã‚¸ãƒ§ãƒ–ã‚’æ‰‹å‹•å®Ÿè¡Œ
gcloud scheduler jobs run daily-diary-generator \
    --location=us-central1 \
    --project=ai-agentic-hackathon-4

# ãƒ­ã‚°ç¢ºèª
gcloud logging read \
    'resource.type="cloud_run_revision" AND textPayload:"diary"' \
    --limit=50 \
    --format=json \
    --project=ai-agentic-hackathon-4
```

#### 3.2 ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆ

```bash
# backend/test_diary_local.py
import asyncio
from datetime import date, timedelta
from diary_service import process_daily_diary

async def test_generate_diary():
    # æ˜¨æ—¥ã®æ—¥è¨˜ã‚’ç”Ÿæˆ
    yesterday = (date.today() - timedelta(days=1)).isoformat()
    print(f"Generating diary for {yesterday}...")
    
    await process_daily_diary(yesterday)
    print("Done!")

if __name__ == "__main__":
    asyncio.run(test_generate_diary())
```

å®Ÿè¡Œ:
```bash
cd backend
python test_diary_local.py
```

---

## ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š

### OIDCèªè¨¼ã®ä»•çµ„ã¿

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cloud Scheduler    â”‚
â”‚                     â”‚
â”‚ 1. JWTãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ  â”‚
â”‚    (scheduler SA)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ POST + Bearer Token
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Endpoint                   â”‚
â”‚                                     â”‚
â”‚ 2. Authorizationãƒ˜ãƒƒãƒ€ãƒ¼ç¢ºèª        â”‚
â”‚ 3. google.oauth2.id_token ã§æ¤œè¨¼   â”‚
â”‚ 4. claim.email ã‚’ç¢ºèª               â”‚
â”‚    â””â”€ scheduler-invoker@ ã®ã¿è¨±å¯  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### IPåˆ¶é™ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

ã‚ˆã‚Šå³æ ¼ãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãŒå¿…è¦ãªå ´åˆï¼š

```python
ALLOWED_IPS = [
    # Google Cloud Scheduler IP ranges
    # https://cloud.google.com/scheduler/docs/reference/rest
]

def verify_ip(request: Request):
    client_ip = request.client.host
    if client_ip not in ALLOWED_IPS:
        raise HTTPException(status_code=403, detail="IP not allowed")
```

---

## é‹ç”¨ãƒ»ç›£è¦–

### Cloud Loggingã‚¯ã‚¨ãƒª

```bash
# æ—¥è¨˜ç”Ÿæˆã®æˆåŠŸãƒ­ã‚°
resource.type="cloud_run_revision"
severity="INFO"
textPayload:"Diary generation completed successfully"

# æ—¥è¨˜ç”Ÿæˆã®å¤±æ•—ãƒ­ã‚°
resource.type="cloud_run_revision"
severity="ERROR"
textPayload:"Diary generation failed"

# Vertex AI Gemini 3 Proå‘¼ã³å‡ºã—
resource.type="cloud_run_revision"
textPayload:"Calling Vertex AI Gemini 3 Pro"

# 429ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼
resource.type="cloud_run_revision"
textPayload:"Rate limit (429) hit"
```

### Cloud Monitoringãƒ¡ãƒˆãƒªã‚¯ã‚¹

ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¿½åŠ :

```python
from google.cloud import monitoring_v3
import time

def record_diary_metric(success: bool, duration_ms: int):
    """æ—¥è¨˜ç”Ÿæˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²"""
    client = monitoring_v3.MetricServiceClient()
    project_name = f"projects/ai-agentic-hackathon-4"
    
    series = monitoring_v3.TimeSeries()
    series.metric.type = "custom.googleapis.com/diary/generation_duration"
    series.resource.type = "global"
    
    point = monitoring_v3.Point()
    point.value.int64_value = duration_ms
    point.interval.end_time.seconds = int(time.time())
    
    series.points = [point]
    
    client.create_time_series(name=project_name, time_series=[series])
```

### ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

```bash
# Cloud Monitoring ã‚¢ãƒ©ãƒ¼ãƒˆãƒãƒªã‚·ãƒ¼ä½œæˆä¾‹
gcloud alpha monitoring policies create \
    --notification-channels=CHANNEL_ID \
    --display-name="Diary Generation Failure Alert" \
    --condition-display-name="Diary generation failed" \
    --condition-threshold-value=1 \
    --condition-threshold-duration=300s \
    --condition-filter='resource.type="cloud_run_revision" AND severity="ERROR" AND textPayload:"Diary generation failed"'
```

---

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### å•é¡Œ1: Schedulerã‚¸ãƒ§ãƒ–ãŒå®Ÿè¡Œã•ã‚Œãªã„

**ç—‡çŠ¶**: 23:50ã«ãªã£ã¦ã‚‚ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒå‘¼ã°ã‚Œãªã„

**ç¢ºèªé …ç›®**:
```bash
# ã‚¸ãƒ§ãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
gcloud scheduler jobs describe daily-diary-generator \
    --location=us-central1

# æœ€è¿‘ã®å®Ÿè¡Œå±¥æ­´
gcloud scheduler jobs describe daily-diary-generator \
    --location=us-central1 \
    --format="value(state, scheduleTime, status)"
```

**è§£æ±ºç­–**:
- ã‚¸ãƒ§ãƒ–ãŒæœ‰åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
- ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å¼ãŒæ­£ã—ã„ã‹ç¢ºèª
- ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³è¨­å®šç¢ºèª

### å•é¡Œ2: 403 Forbidden ã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶**: ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¯å‘¼ã°ã‚Œã‚‹ãŒ403ã‚¨ãƒ©ãƒ¼

**ç¢ºèªé …ç›®**:
```bash
# IAMæ¨©é™ç¢ºèª
gcloud run services get-iam-policy ai-batake-app \
    --region=us-central1 \
    --format=json
```

**è§£æ±ºç­–**:
```bash
# æ¨©é™ã‚’å†ä»˜ä¸
gcloud run services add-iam-policy-binding ai-batake-app \
    --member="serviceAccount:scheduler-invoker@PROJECT_ID.iam.gserviceaccount.com" \
    --role="roles/run.invoker" \
    --region=us-central1
```

### å•é¡Œ3: Vertex AI Gemini API ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ

**ç—‡çŠ¶**: æ—¥è¨˜ç”ŸæˆãŒé€”ä¸­ã§å¤±æ•—

**ãƒ­ã‚°ä¾‹**:
```
ERROR: API request timeout after retries
```

**è§£æ±ºç­–**:
- ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’å¢—ã‚„ã™ï¼ˆ`max_retries=5`ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ã‚’å»¶é•·ï¼ˆ90ç§’ â†’ 120ç§’ï¼‰
- ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ—¥è¨˜ç”ŸæˆãŒæ­£ã—ãå‹•ä½œã™ã‚‹ã‹ç¢ºèª
- Vertex AI APIã®ã‚¯ã‚©ãƒ¼ã‚¿ç¢ºèª

### å•é¡Œ4: 429 ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶**: 429ã‚¨ãƒ©ãƒ¼ãŒé »ç™º

**ãƒ­ã‚°ä¾‹**:
```
WARNING: Rate limit (429) hit. Waiting 4.5s before retry (attempt 2/5)...
```

**ç¢ºèªé …ç›®**:
- Vertex AI APIã®ã‚¯ã‚©ãƒ¼ã‚¿è¨­å®šã‚’ç¢ºèª
- åŒæ™‚å®Ÿè¡Œæ•°ãŒå¤šã™ããªã„ã‹ç¢ºèª
- ãƒªãƒˆãƒ©ã‚¤è¨­å®šãŒé©åˆ‡ã‹ç¢ºèª

**è§£æ±ºç­–**:
```bash
# Vertex AIã®ã‚¯ã‚©ãƒ¼ã‚¿ã‚’ç¢ºèª
gcloud services list --enabled | grep aiplatform

# ã‚¯ã‚©ãƒ¼ã‚¿ã®å¢—åŠ ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
# Google Cloud Console > IAM & Admin > Quotas
```

æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼ˆbase_delay=2ç§’ã€max_retries=5ï¼‰ã«ã‚ˆã‚Šï¼š
- 1å›ç›®ãƒªãƒˆãƒ©ã‚¤: 2-3ç§’å¾…æ©Ÿ
- 2å›ç›®ãƒªãƒˆãƒ©ã‚¤: 4-5ç§’å¾…æ©Ÿ
- 3å›ç›®ãƒªãƒˆãƒ©ã‚¤: 8-9ç§’å¾…æ©Ÿ
- 4å›ç›®ãƒªãƒˆãƒ©ã‚¤: 16-17ç§’å¾…æ©Ÿ
- 5å›ç›®ãƒªãƒˆãƒ©ã‚¤: 32-33ç§’å¾…æ©Ÿ

åˆè¨ˆæœ€å¤§ç´„65ç§’ã®ãƒªãƒˆãƒ©ã‚¤æœŸé–“ãŒã‚ã‚Šã¾ã™ã€‚

### å•é¡Œ5: ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã„

**ç—‡çŠ¶**: ã‚»ãƒ³ã‚µãƒ¼ãƒ­ã‚°ã¾ãŸã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ­ã‚°ãŒ0ä»¶

**ç¢ºèªé …ç›®**:
```python
# Firestoreã‚¯ã‚¨ãƒªã‚’ãƒ†ã‚¹ãƒˆ
from google.cloud import firestore
db = firestore.Client()

# ã‚»ãƒ³ã‚µãƒ¼ãƒ­ã‚°ç¢ºèª
sensor_logs = db.collection("sensor_logs").limit(5).stream()
for log in sensor_logs:
    print(log.to_dict())

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ­ã‚°ç¢ºèª
agent_logs = db.collection("agent_execution_logs").limit(5).stream()
for log in agent_logs:
    print(log.to_dict())
```

**è§£æ±ºç­–**:
- ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åãŒæ­£ã—ã„ã‹ç¢ºèª
- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒä½œæˆã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªï¼ˆFirestoreã‚³ãƒ³ã‚½ãƒ¼ãƒ«ï¼‰
- æ—¥ä»˜ç¯„å›²ã‚¯ã‚¨ãƒªã®æ¡ä»¶ã‚’ç¢ºèª

---

## ã¾ã¨ã‚

### å®Ÿè£…å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

#### ã‚¤ãƒ³ãƒ•ãƒ©è¨­å®š
- [ ] ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆå®Œäº†
- [ ] IAMæ¨©é™ä»˜ä¸å®Œäº†
- [ ] Cloud Schedulerã‚¸ãƒ§ãƒ–ä½œæˆå®Œäº†
- [ ] æ‰‹å‹•å®Ÿè¡Œãƒ†ã‚¹ãƒˆæˆåŠŸ

#### ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
- [ ] `diary_service.py` å®Ÿè£…å®Œäº†
- [ ] `main.py` ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¿½åŠ å®Œäº†
- [ ] OIDCèªè¨¼å®Ÿè£…å®Œäº†
- [ ] ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å®Ÿè£…å®Œäº†

#### ãƒ†ã‚¹ãƒˆ
- [ ] æ‰‹å‹•ãƒˆãƒªã‚¬ãƒ¼ãƒ†ã‚¹ãƒˆæˆåŠŸ
- [ ] ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆæˆåŠŸ
- [ ] æ—¥è¨˜ãƒ‡ãƒ¼ã‚¿ãŒFirestoreã«ä¿å­˜ç¢ºèª
- [ ] ãƒ­ã‚°ãŒæ­£ã—ãå‡ºåŠ›ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª

#### é‹ç”¨æº–å‚™
- [ ] Cloud Loggingã‚¯ã‚¨ãƒªä½œæˆ
- [ ] ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®šå®Œäº†
- [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å‚™å®Œäº†

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰å®Ÿè£…**: `/diary` ãƒšãƒ¼ã‚¸ã®ä½œæˆ
2. **UI/UXæ”¹å–„**: ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ“ãƒ¥ãƒ¼ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ©Ÿèƒ½
3. **æ©Ÿèƒ½æ‹¡å¼µ**: é€±æ¬¡ãƒ»æœˆæ¬¡ãƒ¬ãƒãƒ¼ãƒˆã€PDFå‡ºåŠ›
4. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**: ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã€ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³

---

**ä½œæˆæ—¥**: 2025-02-04  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0  
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: å®Ÿè£…æº–å‚™å®Œäº†
