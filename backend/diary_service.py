import os
import logging
import json
import time
import random
import httpx
import asyncio
from datetime import datetime, date, timedelta
from typing import Dict, List, Any, Optional

import google.auth
import google.auth.transport.requests
from google.cloud.firestore_v1.base_query import FieldFilter

# Setup logging
try:
    from .logger import info, debug, error, warning
except ImportError:
    from logger import info, debug, error, warning

try:
    from .db import db, get_agent_execution_logs, get_sensor_history, get_latest_vegetable, get_edge_agent_config, col
    from .image_service import generate_picture_diary
except ImportError:
    from db import db, get_agent_execution_logs, get_sensor_history, get_latest_vegetable, get_edge_agent_config, col
    from image_service import generate_picture_diary


async def get_auth_headers_async():
    """Get authentication headers for Gemini API."""
    api_key = os.environ.get("SEED_GUIDE_GEMINI_KEY")
    if api_key:
        return {"Content-Type": "application/json"}, f"?key={api_key}"
    
    logging.warning("Please set SEED_GUIDE_GEMINI_KEY.")
    return {"Content-Type": "application/json"}, ""


async def request_with_retry_async(method: str, url: str, **kwargs) -> httpx.Response:
    """Make HTTP request with full-jitter retry for rate limiting using httpx.
    Retries up to 30 minutes with randomized delay (1~max_delay s).
    """
    max_retries = 100
    max_elapsed_seconds = 1800  # 30 minutes
    max_delay = 15.0
    min_delay = 1.0
    start_time = time.time()
    
    last_status_code = None
    last_exc: Exception | None = None
    async with httpx.AsyncClient() as client:
        for i in range(max_retries):
            elapsed = time.time() - start_time
            if elapsed >= max_elapsed_seconds:
                warning(f"[LLM] â° Retry budget exceeded ({max_elapsed_seconds}s). Last status: {last_status_code}")
                break
            try:
                info(f"[LLM] ðŸ”„ API Request attempt {i+1}/{max_retries}: {method} {url[:80]}... (elapsed={elapsed:.0f}s)")
                response = await client.request(method, url, **kwargs)
                last_status_code = response.status_code
                info(f"[LLM] API Response status: {last_status_code}")
                
                if response.status_code == 429 or response.status_code >= 500:
                    sleep_time = random.uniform(min_delay, max_delay)
                    warning(f"API {response.status_code}. Retrying in {sleep_time:.2f}s... (Attempt {i+1}/{max_retries})")
                    await asyncio.sleep(sleep_time)
                    continue
                return response
            except httpx.RequestError as e:
                last_exc = e
                sleep_time = random.uniform(min_delay, max_delay)
                warning(f"Request failed: {e}. Retrying in {sleep_time:.2f}s... (Attempt {i+1}/{max_retries})")
                await asyncio.sleep(sleep_time)
        else:
            # for-loop exhausted without break â€“ do a final attempt
            info("[LLM] Final API Request attempt...")
            return await client.request(method, url, **kwargs)

    # Reached here only via 'break' (time budget exceeded) or loop exhaustion
    if last_status_code == 429:
        raise RuntimeError("AI model rate limit exceeded (429). Please try again later.")
    if last_exc:
        raise last_exc
    raise httpx.HTTPStatusError(
        f"Retry budget exhausted after {max_elapsed_seconds}s (Status: {last_status_code})",
        request=httpx.Request(method, url),
        response=response,  # noqa: F821
    )


async def get_agent_logs_for_date_async(target_date: date) -> List[Dict]:
    """Get agent execution logs for a specific date (async)."""
    if db is None:
        logging.warning("Database not available")
        return []
    
    try:
        # Note: get_agent_execution_logs is currently sync in db.py? 
        # Actually in main.py it's imported from db.py
        # Let's assume we use asyncio.to_thread if it's sync, or if db.py is updated.
        # Based on my view of main.py, it says "Firestore Client (Async)" but imports sync functions?
        # Re-checking db.py: it uses 'firestore.Client' (sync). 
        # Re-checking main.py: line 144 'db = firestore.AsyncClient(...)'.
        # This is a bit mixed. I will use asyncio.to_thread for now for safety or direct calls if async.
        
        all_logs = await asyncio.to_thread(get_agent_execution_logs, limit=1000)
        
        filtered_logs = []
        for log in all_logs:
            timestamp_str = log.get("timestamp", "")
            if timestamp_str:
                try:
                    log_date = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00")).date()
                    if log_date == target_date:
                        filtered_logs.append(log)
                except (ValueError, TypeError):
                    continue
        
        return filtered_logs
    except Exception as e:
        logging.error(f"Error fetching agent logs for date: {e}")
        return []


async def get_sensor_data_for_date_async(target_date: date) -> List[Dict]:
    """Get sensor data for a specific date (async)."""
    if db is None:
        logging.warning("Database not available")
        return []
    
    try:
        # Calculate hours back to the start of the target date
        now = datetime.now()
        start_of_day = datetime.combine(target_date, datetime.min.time())
        hours_back_to_start = int((now - start_of_day).total_seconds() / 3600) + 1
        
        # Buffer to ensure we get everything
        debug(f"Fetching sensor history for {target_date} (hours back: {hours_back_to_start})")
        all_data = await asyncio.to_thread(get_sensor_history, hours=max(hours_back_to_start, 24))
        
        filtered_data = []
        for reading in all_data:
            unix_ts = reading.get("unix_timestamp")
            if unix_ts:
                reading_date = datetime.fromtimestamp(unix_ts).date()
                if reading_date == target_date:
                    filtered_data.append(reading)
        
        debug(f"Retrieved {len(filtered_data)} sensor records for {target_date}")
        return filtered_data
    except Exception as e:
        logging.error(f"Error fetching sensor data for date: {e}")
        return []


async def get_current_vegetable_async() -> Optional[Dict]:
    """Get information about the currently growing vegetable (async)."""
    try:
        # First try to get from edge agent config (active vegetable)
        config = await asyncio.to_thread(get_edge_agent_config)
        if config and config.get("vegetable_name"):
            # Return a dict that mimics the vegetable object, at least with the name
            # We might miss the ID if we don't look it up, but for the diary prompt, name is key.
            return {
                "name": config.get("vegetable_name"),
                "id": None, # ID is unknown without lookup, but acceptable for diary content
                "status": "active_in_config"
            }

        return await asyncio.to_thread(get_latest_vegetable)
    except Exception as e:
        logging.error(f"Error fetching current vegetable: {e}")
        return None


async def get_plant_image_for_date_async(target_date: date) -> Optional[str]:
    """Get the plant camera image URL for a specific date (async)."""
    return None


async def get_selected_character_async() -> Optional[Dict]:
    """Get the selected character from Firestore (growing_diaries/Character) (async)."""
    if db is None:
        logging.warning("Database not available for character lookup")
        return None
    try:
        doc = await asyncio.to_thread(
            lambda: db.collection(col("growing_diaries")).document("Character").get()
        )
        if doc.exists:
            data = doc.to_dict()
            info(f"Selected character found: {data.get('name')}")
            return data
        else:
            info("No selected character found in growing_diaries/Character")
            return None
    except Exception as e:
        logging.error(f"Error fetching selected character: {e}")
        return None


def calculate_statistics(sensor_data: List[Dict]) -> Dict:
    """Calculate statistics from sensor data."""
    if not sensor_data:
        return {
            "temperature": {"min": 0, "max": 0, "avg": 0},
            "humidity": {"min": 0, "max": 0, "avg": 0},
            "soil_moisture": {"min": 0, "max": 0, "avg": 0},
        }
    
    temps = [d.get("temperature", 0) for d in sensor_data if d.get("temperature") is not None]
    humids = [d.get("humidity", 0) for d in sensor_data if d.get("humidity") is not None]
    soils = [d.get("soil_moisture", 0) for d in sensor_data if d.get("soil_moisture") is not None]
    
    return {
        "temperature": {
            "min": round(min(temps), 1) if temps else 0,
            "max": round(max(temps), 1) if temps else 0,
            "avg": round(sum(temps) / len(temps), 1) if temps else 0,
        },
        "humidity": {
            "min": round(min(humids), 1) if humids else 0,
            "max": round(max(humids), 1) if humids else 0,
            "avg": round(sum(humids) / len(humids), 1) if humids else 0,
        },
        "soil_moisture": {
            "min": round(min(soils), 1) if soils else 0,
            "max": round(max(soils), 1) if soils else 0,
            "avg": round(sum(soils) / len(soils), 1) if soils else 0,
        },
    }


def extract_key_events(agent_logs: List[Dict], max_events: int = 10) -> List[Dict]:
    """Extract key events from agent execution logs."""
    events = []
    
    for log in agent_logs:
        log_data = log.get("data", {})
        timestamp = log.get("timestamp", "")
        
        if "operation" in log_data:
            for device, op in log_data["operation"].items():
                action = op.get("action", "") if isinstance(op, dict) else str(op)
                if any(keyword in action for keyword in ["ON", "OFF", "èµ·å‹•", "åœæ­¢", "é–‹å§‹", "çµ‚äº†"]):
                    events.append({
                        "time": timestamp,
                        "type": "action",
                        "device": device,
                        "action": action
                    })
        
        comment = log_data.get("comment", "")
        if "ç•°å¸¸" in comment or "ã‚¨ãƒ©ãƒ¼" in comment:
            events.append({"time": timestamp, "type": "alert", "action": comment})
        elif "è­¦å‘Š" in comment or "æ³¨æ„" in comment:
            events.append({"time": timestamp, "type": "warning", "action": comment})
        elif comment and comment not in [e.get("action") for e in events[-3:] if e]:
            events.append({"time": timestamp, "type": "info", "action": comment[:100]})
    
    return events[:max_events]


def build_diary_prompt(date_str: str, statistics: Dict, events: List[Dict], vegetable_info: Optional[Dict], character_info: Optional[Dict] = None) -> str:
    """Build the prompt for AI diary generation."""
    veg_name = vegetable_info.get("name", "é‡Žèœ") if vegetable_info else "é‡Žèœ"
    event_summary = "\n".join([f"- {e.get('time', 'N/A')}: {e.get('device', '')} {e['action']}" for e in events[:10]])
    
    # Build character instruction
    char_name = None
    char_personality = None
    if character_info:
        char_name = character_info.get("name")
        char_personality = character_info.get("personality")
    
    if char_name and char_personality:
        role_instruction = f"""ã‚ãªãŸã¯ã€Œ{char_name}ã€ã¨ã„ã†åå‰ã®{veg_name}ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã§ã™ã€‚
æ€§æ ¼ã¯ã€Œ{char_personality}ã€ã§ã™ã€‚
ã“ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã«ãªã‚Šãã£ã¦ã€ä¸€äººç§°ã§è‚²æˆæ—¥è¨˜ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚
ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®æ€§æ ¼ã‚„å£èª¿ã‚’åæ˜ ã—ãŸã€è¦ªã—ã¿ã‚„ã™ã„æ–‡ä½“ã§æ›¸ã„ã¦ãã ã•ã„ã€‚
**ã€é‡è¦ï¼šè‡ªç„¶ãªç™ºè©±ã€‘**: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã‚’å®ˆã‚Šã¤ã¤ã‚‚ã€**ã‚ã–ã¨ã‚‰ã—ã„æ¼”æŠ€ã‚„éŽå‰°ãªã‚­ãƒ£ãƒ©ä½œã‚Šã¯é¿ã‘ã¦ãã ã•ã„**ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å½¹ã«ç«‹ã¤ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã€ãã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚‰ã—ã„è‡ªç„¶ãªè¨€è‘‰é¸ã³ã§ä¼ãˆã¦ãã ã•ã„ã€‚
**ã€æ•°å€¤ã®è¨€ã„æ›ãˆã€‘**: æ°—æ¸©ï¼ˆâ—¯â„ƒï¼‰ã€æ¹¿åº¦ï¼ˆâ—¯%ï¼‰ã€é£½å·®ï¼ˆâ—¯kPaï¼‰ãªã©ã®å…·ä½“çš„ãªæ•°å€¤ã¯è¨€ã‚ãšã«ã€**å¿…ãšã€Œå°‘ã—è‚Œå¯’ã„ã€ã€Œæ¹¿åº¦ã¯ã¡ã‚‡ã†ã©è‰¯ã„ã€ã€Œä¹¾ç‡¥ã—ã¦ãã¦ã„ã‚‹ã€ã®ã‚ˆã†ã«ã€ä½“æ„Ÿã‚„çŠ¶æ…‹ã‚’è¡¨ã™è¨€è‘‰ã«è¨€ã„æ›ãˆã¦ä¼ãˆã¦ãã ã•ã„**ã€‚
"""
    elif char_name:
        role_instruction = f"""ã‚ãªãŸã¯ã€Œ{char_name}ã€ã¨ã„ã†åå‰ã®{veg_name}ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã§ã™ã€‚
ã“ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã«ãªã‚Šãã£ã¦ã€ä¸€äººç§°ã§è¦ªã—ã¿ã‚„ã™ã„è‚²æˆæ—¥è¨˜ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚"""
    else:
        role_instruction = "ã‚ãªãŸã¯æ¤ç‰©æ ½åŸ¹ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚‚ã¨ã«ã€è‚²æˆæ—¥è¨˜ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"

    return f"""{role_instruction}

ã€æ—¥ä»˜ã€‘
{date_str}

ã€è‚²æˆä¸­ã®æ¤ç‰©ã€‘
{veg_name}

ã€ç’°å¢ƒãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã€‘
æ¸©åº¦: æœ€ä½Ž {statistics['temperature']['min']}Â°C / æœ€é«˜ {statistics['temperature']['max']}Â°C / å¹³å‡ {statistics['temperature']['avg']}Â°C
æ¹¿åº¦: æœ€ä½Ž {statistics['humidity']['min']}% / æœ€é«˜ {statistics['humidity']['max']}% / å¹³å‡ {statistics['humidity']['avg']}%
åœŸå£Œæ°´åˆ†: æœ€ä½Ž {statistics['soil_moisture']['min']}% / æœ€é«˜ {statistics['soil_moisture']['max']}% / å¹³å‡ {statistics['soil_moisture']['avg']}%

ã€ä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆã€‘
{event_summary if event_summary else "ç‰¹ã«ãªã—"}

ä»¥ä¸‹ã®3ã¤ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†ã‘ã¦æ—¥è¨˜ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

1. **ä»Šæ—¥ã®è¦ç´„** (200-300æ–‡å­—)
   - 1æ—¥ã®ç’°å¢ƒçŠ¶æ…‹ã¨å…¨ä½“çš„ãªæ§˜å­ã‚’è¦ç´„
   - ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹ç‰¹å¾´çš„ãªç‚¹ã‚’è¨˜è¼‰

2. **æˆé•·è¦³å¯Ÿ** (100-200æ–‡å­—)
   - æ¤ç‰©ã®çŠ¶æ…‹ã«ã¤ã„ã¦æŽ¨æ¸¬ã•ã‚Œã‚‹è¦³å¯Ÿ
   - ç’°å¢ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åˆ¤æ–­ã§ãã‚‹æˆé•·ã®é€²æ—

3. **æ˜Žæ—¥ã¸ã®ææ¡ˆ** (100-150æ–‡å­—)
   - ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæ”¹å–„ææ¡ˆ
   - æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚„æ³¨æ„ç‚¹

å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆï¼ˆå¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ï¼‰ï¼š
```json
{{
  "summary": "ä»Šæ—¥ã®è¦ç´„æ–‡...",
  "observations": "æˆé•·è¦³å¯Ÿæ–‡...",
  "recommendations": "æ˜Žæ—¥ã¸ã®ææ¡ˆæ–‡..."
}}
```

æ—¥è¨˜ã¯è¦ªã—ã¿ã‚„ã™ãã€å°‚é–€çš„ã™ãŽãªã„æ–‡ä½“ã§æ›¸ã„ã¦ãã ã•ã„ã€‚
"""


def parse_diary_response(text: str) -> Dict[str, str]:
    """Parse the AI response into structured diary content."""
    try:
        clean_text = text.strip()
        if "```json" in clean_text:
            clean_text = clean_text.split("```json")[1].split("```")[0]
        elif "```" in clean_text:
            clean_text = clean_text.split("```")[1].split("```")[0]
        
        parsed = json.loads(clean_text.strip())
        return {
            "summary": parsed.get("summary", ""),
            "observations": parsed.get("observations", ""),
            "recommendations": parsed.get("recommendations", "")
        }
    except Exception as e:
        error(f"Failed to parse AI response: {e}")
        return {
            "summary": text[:300] if text else "ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æžä¸­ã§ã™ã€‚",
            "observations": "ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æžä¸­ã§ã™ã€‚",
            "recommendations": "å¼•ãç¶šãè¦³å¯Ÿã‚’ç¶šã‘ã¾ã™ã€‚"
        }


async def generate_diary_with_ai_async(
    date_str: str,
    statistics: Dict,
    events: List[Dict],
    vegetable_info: Optional[Dict],
    character_info: Optional[Dict] = None
) -> Dict[str, str]:
    """Generate diary content using Gemini AI (async)."""
    api_key = os.environ.get("SEED_GUIDE_GEMINI_KEY") or os.environ.get("GEMINI_API_KEY")
    if not api_key:
        logging.error("No API Key found")
        return {
            "summary": "AIã‚µãƒ¼ãƒ“ã‚¹ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ (API Key Missing)ã€‚",
            "observations": "ãƒ‡ãƒ¼ã‚¿åŽé›†ã¯æ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚",
            "recommendations": "æ‰‹å‹•ã§è¦³å¯Ÿè¨˜éŒ²ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚"
        }
    
    headers = {"Content-Type": "application/json"}
    prompt = build_diary_prompt(date_str, statistics, events, vegetable_info, character_info)
    url = f"https://aiplatform.googleapis.com/v1/publishers/google/models/gemini-3-flash-preview:generateContent?key={api_key}"
    info(f"[LLM] ðŸ“ Generating diary text via gemini-3-flash-preview for {date_str}")
    
    payload = {
        "contents": [{"role": "user", "parts": [{"text": prompt}]}],
        "generationConfig": {
            "temperature": 0.7,
            "topP": 0.9,
            "maxOutputTokens": 4000,
            "responseMimeType": "application/json"
        }
    }
    
    try:
        response = await request_with_retry_async("POST", url, headers=headers, json=payload, timeout=60)
        if response.status_code != 200:
            error(f"Gemini API error: {response.status_code} - {response.text}")
            return {"summary": f"AIç”Ÿæˆã‚¨ãƒ©ãƒ¼ (HTTP {response.status_code})", "observations": "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", "recommendations": "å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"}
        
        result = response.json()
        generated_text = result["candidates"][0]["content"]["parts"][0]["text"]
        info(f"[LLM] âœ… Diary text generated successfully for {date_str}")
        return parse_diary_response(generated_text)
    except Exception as e:
        error(f"Error calling Gemini API: {e}")
        return {"summary": "AIç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", "observations": "ãƒ‡ãƒ¼ã‚¿åŽé›†ã¯æ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚", "recommendations": "æ‰‹å‹•ã§è¦³å¯Ÿè¨˜éŒ²ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚"}


async def collect_daily_data_async(target_date: date, progress_callback=None) -> Dict[str, Any]:
    """Collect all data needed for diary generation (async)."""
    async def update(msg: str):
        if progress_callback:
            if asyncio.iscoroutinefunction(progress_callback):
                await progress_callback(msg)
            else:
                progress_callback(msg)

    await update("ãƒ­ã‚°ã‚’å–å¾—ä¸­...")
    agent_logs = await get_agent_logs_for_date_async(target_date)
    
    await update("ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªä¸­...")
    sensor_data = await get_sensor_data_for_date_async(target_date)
    
    await update("æ¤ç‰©æƒ…å ±ã‚’å–å¾—ä¸­...")
    vegetable = await get_current_vegetable_async()
    
    await update("ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æƒ…å ±ã‚’å–å¾—ä¸­...")
    character = await get_selected_character_async()
    
    plant_image = await get_plant_image_for_date_async(target_date)
    
    return {
        "date": target_date.isoformat(),
        "agent_logs": agent_logs,
        "sensor_data": sensor_data,
        "vegetable": vegetable,
        "character": character,
        "plant_image": plant_image
    }


async def init_diary_status_async(diary_id: str):
    """Initialize diary generation status in Firestore (async)."""
    if db is None: return
    try:
        await asyncio.to_thread(db.collection(col("growing_diaries")).document(diary_id).set, {
            "generation_status": "processing",
            "created_at": datetime.now()
        })
    except Exception as e:
        logging.error(f"Error initializing diary status: {e}")


async def save_diary_async(diary_id: str, data: Dict):
    """Save the generated diary to Firestore (async)."""
    if db is None: return
    try:
        await asyncio.to_thread(db.collection(col("growing_diaries")).document(diary_id).set, data)
        info(f"Diary saved: {diary_id}")
    except Exception as e:
        error(f"Error saving diary: {e}")


async def mark_diary_failed_async(diary_id: str, error_msg: str):
    """Mark diary generation as failed (async)."""
    if db is None: return
    try:
        await asyncio.to_thread(db.collection(col("growing_diaries")).document(diary_id).update, {
            "generation_status": "failed",
            "error_message": error_msg,
            "updated_at": datetime.now()
        })
    except Exception as e:
        logging.error(f"Error marking diary failed: {e}")


async def process_daily_diary(target_date_str: str, progress_callback=None):
    """Main diary generation process (async)."""
    import asyncio
    async def update_progress(msg: str):
        if progress_callback:
            if asyncio.iscoroutinefunction(progress_callback):
                await progress_callback(msg)
            else:
                progress_callback(msg)

    start_time = time.time()
    diary_id = target_date_str
    
    try:
        target_date = date.fromisoformat(target_date_str)
        info(f"[Diary] ðŸ“– Starting diary generation for {target_date_str}...")
        
        await update_progress("ãƒ‡ãƒ¼ã‚¿åŽé›†ä¸­...")
        await init_diary_status_async(diary_id)
        
        daily_data = await collect_daily_data_async(target_date, progress_callback)
        
        await update_progress("ç”Ÿè‚²çŠ¶æ³ã‚’åˆ†æžä¸­...")
        statistics = calculate_statistics(daily_data["sensor_data"])
        events = extract_key_events(daily_data["agent_logs"])
        
        await update_progress("æˆé•·è¨˜éŒ²ã‚’åŸ·ç­†ä¸­...")
        info(f"[LLM] âœï¸ Starting AI diary text generation for {target_date_str}")
        ai_content = await generate_diary_with_ai_async(
            target_date_str, statistics, events, daily_data["vegetable"], daily_data.get("character")
        )
        
        try:
            await update_progress("çµµæ—¥è¨˜ã®ã‚¤ãƒ©ã‚¹ãƒˆã‚’ç”Ÿæˆä¸­...")
            info(f"[LLM] ðŸŽ¨ Starting diary illustration generation for {target_date_str}")
            # Note: generate_picture_diary is sync, using to_thread
            generated_image_url = await asyncio.to_thread(generate_picture_diary, target_date_str, ai_content["summary"])
            if generated_image_url:
                daily_data["plant_image"] = generated_image_url
        except Exception as img_err:
            error(f"Failed to generate picture diary image: {img_err}")
        
        await update_progress("ä¿å­˜ã—ã¦ã„ã¾ã™...")
        generation_time_ms = int((time.time() - start_time) * 1000)
        
        diary_data = {
            "date": target_date_str,
            "created_at": datetime.now(),
            "vegetable_id": daily_data["vegetable"].get("id") if daily_data["vegetable"] else None,
            "vegetable_name": daily_data["vegetable"].get("name") if daily_data["vegetable"] else None,
            "statistics": statistics,
            "events": events,
            "ai_summary": ai_content["summary"],
            "observations": ai_content["observations"],
            "recommendations": ai_content["recommendations"],
            "plant_image_url": daily_data["plant_image"],
            "generation_status": "completed",
            "generation_time_ms": generation_time_ms,
            "agent_actions_count": len(daily_data["agent_logs"])
        }
        
        await save_diary_async(diary_id, diary_data)
        info(f"[Diary] âœ… Diary generated successfully for {target_date_str} in {generation_time_ms}ms")
        
    except Exception as e:
        error_msg = str(e)
        if "429" in error_msg:
            friendly_msg = "AIãƒ¢ãƒ‡ãƒ«ã®åˆ©ç”¨åˆ¶é™ï¼ˆ429: Too Many Requestsï¼‰ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
        else:
            friendly_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_msg}"
            
        error(f"Failed to generate diary for {target_date_str}: {error_msg}")
        await mark_diary_failed_async(diary_id, friendly_msg)


def get_all_diaries(limit: int = 30, offset: int = 0) -> List[Dict]:
    """Get all diaries from Firestore (sync)."""
    if db is None: return []
    try:
        query = db.collection(col("growing_diaries")).where(filter=FieldFilter("generation_status", "==", "completed")).limit(limit)
        if offset > 0: query = query.offset(offset)
        docs = query.stream()
        diaries = []
        for doc in docs:
            diary = doc.to_dict()
            diary['id'] = doc.id
            if 'created_at' in diary and hasattr(diary['created_at'], 'isoformat'):
                diary['created_at'] = diary['created_at'].isoformat()
            diaries.append(diary)
        diaries.sort(key=lambda x: x.get('date', ''), reverse=True)
        return diaries
    except Exception as e:
        error(f"Error fetching diaries: {e}")
        return []


def get_diary_by_date(date_str: str) -> Optional[Dict]:
    """Get a specific diary by date (sync)."""
    if db is None: return None
    try:
        doc = db.collection(col("growing_diaries")).document(date_str).get()
        if not doc.exists: return None
        diary = doc.to_dict()
        diary['id'] = doc.id
        if 'created_at' in diary and hasattr(diary['created_at'], 'isoformat'):
            diary['created_at'] = diary['created_at'].isoformat()
        return diary
    except Exception as e:
        logging.error(f"Error fetching diary: {e}")
        return None
